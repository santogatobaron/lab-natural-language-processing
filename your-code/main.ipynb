{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (C:\\Users\\santi\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[32m      2\u001b[39m display(HTML(\u001b[33m\"\u001b[39m\u001b[33m<style>.container \u001b[39m\u001b[33m{\u001b[39m\u001b[33m width:100\u001b[39m\u001b[33m%\u001b[39m\u001b[33m !important; }</style>\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (C:\\Users\\santi\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display.py)"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n",
      "Train size: (800,)\n",
      "Test size: (200,)\n",
      "\n",
      "distribution in train:\n",
      "label\n",
      "0    0.5575\n",
      "1    0.4425\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "distribution in test:\n",
      "label\n",
      "0    0.56\n",
      "1    0.44\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "\n",
    "\n",
    "# check column names\n",
    "print(data.head())\n",
    "\n",
    "# split between text and label\n",
    "X = data[\"text\"] \n",
    "y = data[\"label\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,       \n",
    "    random_state=42,\n",
    "    # to include similar proportions of spam and mail\n",
    "    stratify=y           \n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "print(\"\\ndistribution in train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\ndistribution in test:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def clean_text_1(text):\n",
    "    # JS\n",
    "    text = re.sub(r\"<script.*?>.*?</script>\", \" \", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    # CSS\n",
    "    text = re.sub(r\"<style.*?>.*?</style>\", \" \", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    # html comments\n",
    "    text = re.sub(r\"<!--.*?-->\", \" \", text, flags=re.DOTALL)\n",
    "    # rest of tags\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "   \n",
    "X_train_clean = X_train.apply(clean_text_1)\n",
    "X_test_clean = X_test.apply(clean_text_1)\n",
    "\n",
    "# print(\"ORIGINAL:\\n\", X_train.iloc[1])\n",
    "# print(\"\\nCLEANED:\\n\", X_train_clean.iloc[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_2(text):\n",
    "    # Remove all special characters, numbers and punctuation (keep letters only)\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    # Remove all single characters (surrounded by spaces)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r\"^\\s*[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "X_train_clean = X_train_clean.apply(clean_text_2)\n",
    "X_test_clean = X_test_clean.apply(clean_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    email_words = text.split()\n",
    "    email_words = [word for word in email_words if word not in stop_words]\n",
    "    return \" \".join(email_words)\n",
    "\n",
    "X_train_clean = X_train_clean.apply(remove_stopwords)\n",
    "X_test_clean = X_test_clean.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL CLEANED:\n",
      " mr henry kaborethe chief auditor inchargeforeign remittance unit african development bank adb ouagadougou burkina faso dear partner presumed well family please let surprise message got contact information fromthe international directory week ago decided contact magintude lucrative transaction present future survival life moreover laid solemn trust decided disclose successful confidential transaction chief auditor incharge foreign remittance unit bank decided contact financial transaction worth sum nineteen million three hundred thousand united state america present future success abandoned fund belongs one bank foreign customer died along entire family plane crash disaster since year ago meanwhile fortune came across deceased file arranging old abandoned customer file sign submit entire bank management official documentation audit year informed clearly stated foreign banking rule regulation signed lawfully fund remains unclaimed till period year started date beneficiary died money transferred treasury unclaimed fund however authorized rule guiding bank citizen burkina faso make claim fund unless foreigner matter country different deceased request foreigner necessary apply claim transfer fund smoothly reliable bank account next kin deceased fund transferred account thirty five percent respect assistance transfer fund account provision bank account bank remit fund fifty five percent pioneer business rest ten percent shared respectable organisation center charity motherless baby home helpless disabled people around u world really sure trustworthy accountability confidentiality transaction contact accept change mind cheat disappoint fund transferred account reply assurance telephone fax number necessary facilitate easy communication soon reply let know next procedure follow order finalize transaction immediately expect urgent communication sincerely mr henry kabore search find check new msn search http search msn com\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "X_train_clean = X_train_clean.apply(lemmatize_text)\n",
    "X_test_clean  = X_test_clean.apply(lemmatize_text)\n",
    "\n",
    "i = 1\n",
    "print(\"FINAL CLEANED:\\n\", X_train_clean.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('state', 97), ('president', 95), ('would', 92), ('mr', 90), ('obama', 80), ('percent', 80), ('call', 77), ('work', 72), ('time', 70), ('one', 69)]\n",
      "[('money', 761), ('account', 674), ('bank', 615), ('fund', 600), ('transaction', 435), ('business', 412), ('country', 401), ('nbsp', 387), ('mr', 384), ('million', 364)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "X_train_ham  = X_train_clean[y_train == 0]\n",
    "X_train_spam = X_train_clean[y_train == 1]\n",
    "\n",
    "# top_10_ham\n",
    "vectorizer_ham = CountVectorizer()\n",
    "X_ham_bow = vectorizer_ham.fit_transform(X_train_ham)\n",
    "\n",
    "ham_word_counts = X_ham_bow.sum(axis=0)\n",
    "ham_words_freq = zip(\n",
    "    vectorizer_ham.get_feature_names_out(),\n",
    "    ham_word_counts.A1\n",
    ")\n",
    "\n",
    "ham_words_freq = sorted(ham_words_freq, key=lambda x: x[1], reverse=True)\n",
    "top_10_ham = ham_words_freq[:10]\n",
    "\n",
    "print(top_10_ham)\n",
    "\n",
    "# top_10_spam\n",
    "vectorizer_spam = CountVectorizer()\n",
    "X_spam_bow = vectorizer_spam.fit_transform(X_train_spam)\n",
    "\n",
    "spam_word_counts = X_spam_bow.sum(axis=0)\n",
    "spam_words_freq = zip(\n",
    "    vectorizer_spam.get_feature_names_out(),\n",
    "    spam_word_counts.A1\n",
    ")\n",
    "\n",
    "spam_words_freq = sorted(spam_words_freq, key=lambda x: x[1], reverse=True)\n",
    "top_10_spam = spam_words_freq[:10]\n",
    "\n",
    "print(top_10_spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()\"\"\"\n",
    "\n",
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", r\"\\$\"])\n",
    "\n",
    "suspicious_words = \"|\".join([\n",
    "    \"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\",\n",
    "    \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"\n",
    "])\n",
    "\n",
    "X_train_money_mark = X_train_clean.str.contains(money_symbol_list, regex=True).astype(int)\n",
    "X_train_suspicious_words = X_train_clean.str.contains(suspicious_words, regex=True).astype(int)\n",
    "X_train_text_len = X_train_clean.apply(len)\n",
    "\n",
    "X_test_money_mark = X_test_clean.str.contains(money_symbol_list, regex=True).astype(int)\n",
    "X_test_suspicious_words = X_test_clean.str.contains(suspicious_words, regex=True).astype(int)\n",
    "X_test_text_len = X_test_clean.apply(len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# train\n",
    "X_train_bow = vectorizer.fit_transform(X_train_clean)\n",
    "# same as \n",
    "# vectorizer.fit(X_train_clean)\n",
    "# X_train_bow = vectorizer.transform(X_train_clean)\n",
    "\n",
    "# test\n",
    "X_test_bow = vectorizer.transform(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# load vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#vectorize dataset\n",
    "#train\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "#test\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       112\n",
      "           1       1.00      0.97      0.98        88\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.99      0.98      0.98       200\n",
      "weighted avg       0.99      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#init classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#train model \n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict for test\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
